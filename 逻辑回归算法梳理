1、逻辑回归与线性回归的联系与区别 
逻辑回归的模型 是一个非线性模型，sigmoid函数，又称逻辑回归函数。但是它本质上又是一个线性回归模型，因为除去sigmoid映射函数关系，其他的步骤，算法都是线性回归的。可以说，逻辑回归，都是以线性回归为理论支持的。只不过，线性模型，无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。另外它的推导含义：仍然与线性回归的最大似然估计推导相同，最大似然函数连续积（这里的分布，可以使伯努利分布，或泊松分布等其他分布形式），求导，得损失函数。

2、 逻辑回归的原理 
Logistic Regression和Linear Regression的原理是相似的，可以简单描述为以下过程：
（1）找一个合适的预测函数，一般表示为h函数，该函数就是我们需要找的分类函数，它用来预测输入数据的判断结果。这个过程时非常关键的，需要对数据有一定的了解或分析，知道或者猜测预测函数的“大概”形式，比如是线性函数还是非线性函数。
（2）构造一个Cost函数（损失函数），该函数表示预测的输出（h）与训练数据类别（y）之间的偏差，可以是二者之间的差（h-y）或者是其他的形式。综合考虑所有训练数据的“损失”，将Cost求和或者求平均，记为J(θ)函数，表示所有训练数据预测值与实际类别的偏差。
（3）显然，J(θ)函数的值越小表示预测函数越准确（即h函数越准确），所以这一步需要做的是找到J(θ)函数的最小值。找函数的最小值有不同的方法，Logistic Regression实现时有的是梯度下降法（Gradient Descent）。

3、逻辑回归损失函数推导及优化 
推导参考：https://blog.csdn.net/qq_28900249/article/details/83104155   优化参考：https://www.jianshu.com/p/45a62c8dbffa

4、 正则化与模型评估指标 
正则化：当模型参数过多时，容易遇到过拟合问题，因此可以在优化目标中加入正则项来控制模型的复杂度，通过惩罚过大的参数来防止过拟合。
模型评估指标：常见的分类指标有准确率，召回率，ROC曲线等，通常用准确率作为一般评估标准，但这个标准的默认假设前提是数据是平衡的，正例、反例的重要性一样，二分类器的阈值是0.5。

5、逻辑回归的优缺点 
优点：
结果通俗易懂，自变量的系数直接与权重挂钩，可以解释为自变量对目标变量的预测价值大小。速度快，效率高，并且容易线上算法实现。
缺点：
目标变量中每个类别对应的样本数量要充足，才支持建模。要注意排除自变量中的共线性问题。异常值会给模型带来很大干扰，应该删除。模型本身不能处理缺失值，所以需要对缺失值进行预处理。模型是线性分类器，容易欠拟合，分类精度不高

6、样本不均衡问题解决办法 
参考：https://blog.csdn.net/zhongjunlang/article/details/79568601

7. sklearn参数
参考：https://blog.csdn.net/liyuan5241/article/details/82084953
